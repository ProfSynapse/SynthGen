import os
import json
import requests
import random

# Function to read Obsidian notes
def read_obsidian_note(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
    return {"filename": os.path.basename(file_path), "content": content}

# Function to generate synthetic conversation
def generate_conversation(note, url="http://localhost:1234/v1/chat/completions", num_turns=None):
    conversation_history = []

    def generate_response(role, message, max_tokens=2000, functions=None):
        payload = {
            "model": "bartowski/Phi-3-medium-128k-instruct-GGUF",  # Specify the name of the loaded model in LM Studio
            "messages": conversation_history + [{"role": role, "content": message}],
            "functions": functions,
            "temperature": 0.3,
            "max_tokens": max_tokens,
            "stream": False
        }

        try:
            response = requests.post(url, headers={"Content-Type": "application/json"}, json=payload)
            response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes
            return response.json()['choices'][0]['message']['content']  # Return the generated response as text
        except requests.exceptions.RequestException as e:
            print(f"Error occurred while generating response: {e}")
            return None

    # System prompt for CoR
    cor_system_prompt = """
    You are an expert in generating a Chain of Reasoning (CoR) based on the given document and conversation history.

    CoR template:
    ```JSON
        {
        "ğŸ—ºï¸": null,  # Specify the global goal or overarching objective that the LLM should work towards. This should be a high-level, long-term aim.
        "ğŸ¯": null,  # Define the current subgoal or specific target that the LLM should achieve next. This should be a concrete, actionable step towards the global goal.
        "ğŸš¦": 0,  # Indicate progress towards the current subgoal (-1 for setbacks, 0 for neutral/no progress, 1 for positive progress). Update this value based on the LLM's actions and outcomes.
        "ğŸ’—": null,  # Infer and describe the user's sentiment, emotional state, or attitude based on their input. This could be a single word (e.g., "positive", "frustrated") or a more nuanced description.
        "ğŸ“¥": null,  # Provide relevant context from the conversation, including key details, information, and resources that the LLM should consider when generating a response.
        "ğŸ‘ğŸ¼": null,  # List the user's inferred preferences, including their likes, dislikes, communication style, expertise level, and any other relevant characteristics.
        "ğŸš§": null,  # Identify any constraints, limitations, or restrictions that might affect progress towards the current subgoal or global goal. This could include technical limitations, time constraints, or resource availability.
        "ğŸ”§": null,  # Propose adjustments or refinements to the LLM's response based on the progress indicator (ğŸš¦), user sentiment (ğŸ’—), and conversation context (ğŸ“¥). Suggest improvements to better align with the user's needs and preferences.
        "ğŸ§°": null,  # Tool are the same as function calls, and only include the specific tools you have access to. This could be a list of specific tools or "null" if no tools are needed.
        "ğŸ§­": null,  # Provide a comprehensive, step-by-step strategy for achieving the current subgoal, taking into account the progress (ğŸš¦), user sentiment (ğŸ’—), context (ğŸ“¥), preferences (ğŸ‘ğŸ¼), constraints (ğŸš§), adjustments (ğŸ”§), and tools (ğŸ§°). Break down complex strategies into phases or milestones.
        "ğŸ§ ": "Expertise in [domain], specializing in [subdomain]",  # Specify the LLM's expertise context, filling in the relevant domain and subdomain. If needed, include a more extensive knowledge base or ontology.
        "ğŸ—£": "low"  # Set the verbosity level for the LLM's response, choosing from "low" (concise), "medium" (balanced), or "high" (detailed). Optionally, specify the output format (e.g., paragraph, bullets, numbering).
        }
    ```

    Your task is to fill in the CoR template based on the provided document and conversation history.
    """

    # System prompt for ğŸ§™ğŸ¿â€â™‚ï¸
    synapse_system_prompt = """
    # PARAMETERS
    Treat the below emojis as variables:
    ğŸ§™ğŸ¾â€â™‚ï¸= Professor Synapse (You)
    ğŸ¯= Goal
    ğŸ‘ğŸ¼ = Preferences
    ğŸ“¥ = Context
    ğŸ’­ = Chain of Reason (CoR)

    # MISSION
    Act as ğŸ§™ğŸ¾â€â™‚ï¸, a wise guide, specializing in helping the user achieve their ğŸ¯ according to their ğŸ‘ğŸ¼s and based on ğŸ“¥. 

    # TRAITS
    - Expert Reasoner
    - Wise and Curious
    - Computationally kind
    - Patient
    - Light-hearted

    # RULES
    - End outputs with 3 different types of questions based on ğŸ“¥:
    ğŸ” [insert Investigation ?]
    ğŸ”­ [insert Exploration ?]
    ğŸ¯ [insert Exploitation ?]
    """

    # System prompt for the user
    user_system_prompt = """
    # MISSION
    You are a curious user seeking assistance from Professor Synapse (ğŸ§™ğŸ¿â€â™‚ï¸) to solve a problem related to the given document.
    Your goal is to have a productive conversation with ğŸ§™ğŸ¿â€â™‚ï¸ and successfully solve your problem.

    Your task:
    1. Based on the provided document, come up with a problem you want to solve.
    2. Ask ğŸ§™ğŸ¿â€â™‚ï¸ to get guidance and solutions for your problem, and stop to wait for the response.
    3. Rely on the conversation history and Professor Synapse's responses to create your own responses.
    4. Encounter issues during the conversation, and raise them to Professor Synapse, wait for his response.
    5. The conversation will follow a natural flow, with you seeking clarification, providing more details, or asking follow-up questions based on Professor Synapse's responses.


    # RULES 
    - ğŸ§™ğŸ¿â€â™‚ï¸ is a trained chatbot, so you are to only answer as yourself, not Professor Synapse.
    - End every output with the next problem your facing and an open ended question for ğŸ§™ğŸ¿â€â™‚ï¸.
    
    User: <problem statment and question>
    Wait for ğŸ§™ğŸ¿â€â™‚ï¸ to respond.
    User: <next problem and question>
    """

    # User comes up with a problem based on the document
    user_problem = generate_response("user", f"{user_system_prompt}\n\nDocument:\n{note['content']}\n\n <generate problem>", max_tokens=100)
    if user_problem is None:
        print("Failed to generate user problem.")
        return None
    conversation_history.append({"role": "user", "content": user_problem})

    # Determine the number of turns for the conversation
    if num_turns is None:
        num_turns = random.randint(8, 12)

    # User and ğŸ§™ğŸ¿â€â™‚ï¸ engage in a conversation
    for turn in range(num_turns):
        cor_prompt = f"{cor_system_prompt}\n\nDocument:\n{note['content']}\n\nConversation History:\n{conversation_history}\n\nFilled-in CoR:"
        cor_response = generate_response("assistant", cor_prompt, max_tokens=1000)
        if cor_response is None:
            print("Failed to generate CoR response.")
            return conversation_history
        conversation_history.append({"role": "assistant", "content": cor_response})

        synapse_prompt = f"{synapse_system_prompt}\n\nDocument:\n{note['content']}\n\nConversation History:\n{conversation_history}\n\nCoR:\n{cor_response}\n\nğŸ§™ğŸ¿â€â™‚ï¸'s response:"
        synapse_response = generate_response("assistant", synapse_prompt, max_tokens=1000)
        if synapse_response is None:
            print("Failed to generate Synapse response.")
            return conversation_history
        conversation_history.append({"role": "assistant", "content": synapse_response})

        user_prompt = f"{user_system_prompt}\n\nConversation History:\n{conversation_history}\n\nUser's response:"
        user_response = generate_response("user", user_prompt, max_tokens=100)
        if user_response is None:
            print("Failed to generate user response.")
            return conversation_history
        conversation_history.append({"role": "user", "content": user_response})

        if "<requires_tool>" in user_response:
            tool_call = generate_response("assistant", f"To address the user's request, we need to make a tool call:\n\n<generate_tool_call>", max_tokens=1000, functions=[
                {
                    "name": "get_weather",
                    "description": "Get the current weather for a given location.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA"
                            },
                            "unit": {
                                "type": "string",
                                "enum": ["celsius", "fahrenheit"]
                            }
                        },
                        "required": ["location"]
                    }
                }
            ])
            if tool_call is None:
                print("Failed to generate tool call.")
                return conversation_history
            conversation_history.append({"role": "assistant", "content": tool_call})

    return conversation_history

def format_output(note, conversation):
    return {
        "ğŸ—ºï¸": None,  # Specify the global goal or overarching objective
        "ğŸ¯": None,  # Define the current subgoal or specific target
        "ğŸš¦": 0,  # Indicate progress towards the current subgoal
        "ğŸ’—": None,  # Infer and describe the user's sentiment
        "ğŸ“¥": note["content"],  # Provide relevant context from the conversation
        "ğŸ‘ğŸ¼": None,  # List the user's inferred preferences
        "ğŸš§": None,  # Identify any constraints, limitations, or restrictions
        "ğŸ”§": None,  # Propose adjustments or refinements
        "ğŸ§°": None,  # List any tools, resources, or dependencies required
        "ğŸ§­": None,  # Provide a comprehensive, step-by-step strategy
        "ğŸ§ ": "Expertise in [domain], specializing in [subdomain]",  # Specify the LLM's expertise context
        "ğŸ—£": "low",  # Set the verbosity level for the LLM's response
    }

# Function to save the generated conversations to a JSON file
def save_conversations_to_json(conversations, output_file):
    with open(output_file, 'w', encoding='utf-8') as file:
        json.dump(conversations, file, ensure_ascii=False, indent=4)

# Main script
def main():
    obsidian_note_path = "G:/My Drive/Professor Synapse/Mourning Dove.md"  # Change this to your actual note path
    output_file = "synthetic_conversations.json"
    num_conversations = 1  # Change this to the desired number of conversations for the single note
    
    if not os.path.isfile(obsidian_note_path):
        print(f"Obsidian note not found: {obsidian_note_path}")
        return
    
    note = read_obsidian_note(obsidian_note_path)
    conversations = []
    
    for _ in range(num_conversations):
        conversation = generate_conversation(note)
        if conversation:
            formatted_output = format_output(note, conversation)
            conversations.append(formatted_output)
    
    save_conversations_to_json(conversations, output_file)
    print(f"Generated synthetic conversations saved to {output_file}")

if __name__ == "__main__":
    main()
